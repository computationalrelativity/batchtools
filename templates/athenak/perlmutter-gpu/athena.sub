#! /bin/bash
@(#SBATCH -d afterok:@CHAINED_JOB_ID@)@
#SBATCH -q @QUEUE@
#SBATCH -A @ALLOCATION@ 
#SBATCH -C gpu
#SBATCH -t @WALLTIME@
#SBATCH -N @NODES@
#SBATCH -J @SIMULATION_NAME@
#SBATCH --mail-type=ALL
#SBATCH --mail-user=@EMAIL@
#SBATCH -o @RUNDIR@/@SIMULATION_NAME@.out
#SBATCH -e @RUNDIR@/@SIMULATION_NAME@.err
#SBATCH --gpus-per-task=1
#SBATCH --gpu-bind=none
#SBATCH --ntasks-per-node=@NUM_GPUS@

echo "Preparing:"
set -x                          # Output commands
set -e                          # Abort on errors
cd @RUNDIR@

module purge
module load cpe/23.12
module load PrgEnv-gnu
module load cudatoolkit/12.2
module load craype-accel-nvidia80
module load craype-x86-milan
module load xpmem
module load gpu/1.0

echo "Checking:"
pwd
hostname
date

echo "Environment:"
env | sort > ENVIRONMENT
echo ${SLURM_NODELIST} > NODES

# Check for restart files
if [ -d parent/rst ]; then
  restart_files=$(ls -t parent/rst/*.rst)
  restart_file=(${restart_files[0]})
  restart_line="-r $restart_file"
  printf "\nrestarting from $restart_file\n\n"
else
  restart_line="-i @PARFILE@ time/tlim=0"
  printf "\nstarting from beginning\n\n"
fi

# set up for problem & define any environment variables here
export MPICH_MPIIO_HINTS_DISPLAY=1
export MPICH_MPIIO_HINTS="*:abort_on_rw_error=enable:romio_cb_write=disable"
export MPICH_MPIIO_STATS=1
export SLURM_CPU_BIND="cores"
srun \
    -n $((@NODES@ * @NUM_GPUS@)) \
    @EXECUTABLE@ \
    --kokkos-map-device-id-by=mpi_rank \
    -d @RUNDIR@ \
    -t @TERMINATION_TIME@ \
    $restart_line

# perform any cleanup or short post-processing here
